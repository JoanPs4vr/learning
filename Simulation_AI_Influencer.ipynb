# -*- coding: utf-8 -*-
"""Simulation AI Influencer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FiztA4bA7SeYUWtv5UxWjdUShCj6Bph3

**SIMULATION CASE 1: Market Sentiment Analysis of AI Influencers**
**Joan P**


1. Importing the Libraries
"""

# Importing the required the libraries

# To read and manipulate the data
import pandas as pd
pd.set_option('max_colwidth', None)

# To visualise the graphs
import matplotlib.pyplot as plt
import seaborn as sns

# Helps to display the images
from PIL import Image

# Helps to extract the data using regular expressions
import re

# Helps to remove the punctuation
import string

# It helps to remove the accented characters
!pip install unidecode
import unidecode

# Importing the NLTK library
import nltk

nltk.download('stopwords')    # Loading the stopwords
nltk.download('punkt')        # Loading the punkt module, used in Tokenization
nltk.download('omw-1.4')      # Dependency for Tokenization
nltk.download('wordnet')      # Loading the wordnet module, used in stemming and lemmatization

# downloading vader lexicon
nltk.download('vader_lexicon')

from nltk.corpus import stopwords

# Helps to visualize the wordcloud
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

# Used in Stemming
from nltk.stem.porter import PorterStemmer


# Used in Lemmatization
from nltk.stem import WordNetLemmatizer


from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words
from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF

# Helped to create train and test data
from sklearn.model_selection import train_test_split

# Importing the Random Forest model
from sklearn.ensemble import RandomForestClassifier

# Metrics to evaluate the model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Unsupervised learning models

# Install vader sentiment package
!pip install vaderSentiment

# Install textblob package
!pip install textblob

import numpy as np

from google.colab import drive
drive.mount('/content/drive')
excel_path = "/gab_sc1_dataset3.xlsx"
df = pd.read_excel(excel_path)
data = pd.read_excel(excel_path)
print(df.head())

# View the first and last 5 rows of the dataset
data.head(2)

# Understand the shape of the dataset
data.shape

# Check the data types of the columns for the dataset
data.info()

data['Domisili'].value_counts()
data['Gender'].value_counts()
data ['Pekerjaan'].value_counts()
data['Pendidikan'].value_counts()

#Central tendencies of age
m1 = data['Usia'].mean()
print(m1)
m2 = data['Usia'].median()
print(m2)
m3 = data['Usia'].mode()[0]
print(m3)
m4 = data['Usia'].max()
print(m4)
m5 = data['Usia'].min()
print(m5)

# Gender Distribution
plt.figure(figsize=(8, 5))
bars = df["Gender"].value_counts().plot(kind="bar", color=["blue", "pink"], edgecolor="black")
plt.title("Gender Distribution")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.xticks(rotation=0)

# Add count labels
for bar in bars.patches:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), int(bar.get_height()), ha='center', va='bottom', fontsize=12)

plt.show()

# Age Distribution
plt.figure(figsize=(8, 5))
counts, bins, bars = plt.hist(df["Usia"], bins=10, edgecolor="black", alpha=0.7, color="skyblue")
plt.title("Age Distribution")
plt.xlabel("Age")
plt.ylabel("Number of Respondents")
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Add count labels on bars
for bar, count in zip(bars, counts):
    plt.text(bar.get_x() + bar.get_width()/2, count, int(count), ha='center', va='bottom', fontsize=12)

plt.show()

# Occupation Breakdown
plt.figure(figsize=(10, 5))
bars = df["Pekerjaan"].value_counts().plot(kind="bar", color="lightcoral", edgecolor="black")
plt.title("Occupation Breakdown")
plt.xlabel("Occupation")
plt.ylabel("Count")
plt.xticks(rotation=45)

# Add count labels
for bar in bars.patches:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), int(bar.get_height()), ha='center', va='bottom', fontsize=12)

plt.show()

# Education Level Breakdown
plt.figure(figsize=(10, 5))
bars = df["Pendidikan"].value_counts().plot(kind="bar", color="seagreen", edgecolor="black")
plt.title("Education Level Breakdown")
plt.xlabel("Education Level")
plt.ylabel("Count")
plt.xticks(rotation=45)

# Add count labels
for bar in bars.patches:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), int(bar.get_height()), ha='center', va='bottom', fontsize=12)

plt.show()

# Respondent Locations (Domisili)
plt.figure(figsize=(10, 5))
bars = df["Domisili"].value_counts().plot(kind="bar", color="orange", edgecolor="black")
plt.title("Respondent Locations")
plt.xlabel("Location")
plt.ylabel("Count")
plt.xticks(rotation=45)

# Add count labels
for bar in bars.patches:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), int(bar.get_height()), ha='center', va='bottom', fontsize=12)

plt.show()

# Create the count plot
ax = sns.countplot(x="Pekerjaan", hue="Gender", data=df)

# Add count labels on top of each bar
for p in ax.patches:
    ax.annotate(
        format(int(p.get_height()), ','),
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha='center', va='bottom',
        fontsize=12
    )

# Titles and labels
plt.title("Occupation Breakdown by Gender")
plt.xlabel("Occupation")
plt.ylabel("Count")
plt.xticks(rotation=45, ha='right')

# Show the plot
plt.show()

print(df.isnull().sum())

df_cleaned = df.drop(columns=["Influencer _Idola_Lainnya"])
df["Influencer _Idola_Lainnya"] = df["Influencer _Idola_Lainnya"].fillna("Unknown")
df["Influencer _Idola_Lainnya"] = df["Influencer _Idola_Lainnya"].fillna(df["Influencer _Idola_Lainnya"].mode()[0])
df_cleaned = df.dropna(subset=["Influencer _Idola_Lainnya"])

print(df.isnull().sum())

# Remove "Belum ada di daftar" from the Influencer_Idola column
df_filtered = df[df["Influencer_Idola"] != "Belum ada di daftar"]

# Count occurrences of each influencer after filtering
influencer_counts = df_filtered["Influencer_Idola"].value_counts()

# Ensure there are valid influencers before proceeding
if not influencer_counts.empty:
    # Find the most popular influencer
    most_popular = influencer_counts.idxmax()
    most_popular_count = influencer_counts.max()

    print(f"The most popular influencer (excluding 'Belum ada di daftar') is: {most_popular} with {most_popular_count} mentions.")

    # Plot the top 10 most popular influencers
    plt.figure(figsize=(12, 6))
    bars = plt.bar(influencer_counts.index[:10], influencer_counts.values[:10], color="skyblue")

    # Add count labels on top of bars
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)

    # Labels and title
    plt.title("Top 10 Most Popular Influencers (Excluding 'Belum ada di daftar')")
    plt.xlabel("Influencer")
    plt.ylabel("Count")
    plt.xticks(rotation=45, ha="right")
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # Show plot
    plt.show()

else:
    print("No valid influencers found after filtering out 'Belum ada di daftar'.")

# Remove 'Unknown' and NaN values from "Influencer _Idola_Lainnya"
df_filtered = df[(df["Influencer _Idola_Lainnya"].notna()) & (df["Influencer _Idola_Lainnya"] != "Unknown")]

# Count occurrences of each influencer
influencer_lainnya_counts = df_filtered["Influencer _Idola_Lainnya"].value_counts()

# Ensure there are valid influencers before plotting
if not influencer_lainnya_counts.empty:
    # Plot the top 10 most popular "Idola Lainnya" influencers
    plt.figure(figsize=(12, 6))
    bars = plt.bar(influencer_lainnya_counts.index[:10], influencer_lainnya_counts.values[:10], color="lightcoral")

    # Add count labels on top of bars
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)

    # Labels and title
    plt.title("Top 10 Most Popular 'Influencer Idola Lainnya' (Excluding Unknown)")
    plt.xlabel("Influencer (Idola Lainnya)")
    plt.ylabel("Count")
    plt.xticks(rotation=45, ha="right")
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # Show plot
    plt.show()

else:
    print("No valid influencers found in 'Influencer _Idola_Lainnya'.")

# Check if the dataset contains a relevant column for page visits
if "Tengok_Akun_Influencer" in df.columns:
    # Count occurrences of each visit frequency
    visit_counts = df["Tengok_Akun_Influencer"].value_counts().sort_index()

    # Plot the frequency of page visits
    plt.figure(figsize=(10, 6))
    bars = plt.bar(visit_counts.index.astype(str), visit_counts.values, color="royalblue")

    # Add count labels on top of bars
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)

    # Labels and title
    plt.title("Frequency of Page Visits by Visitors")
    plt.xlabel("Number of Times Visitors Open the Page")
    plt.ylabel("Count of Visitors")
    plt.xticks(rotation=0)
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # Show plot
    plt.show()
else:
    print("Column 'Tengok_Akun_Influencer' not found in the dataset.")

# Check if the column exists
if "Daya_Pengaruh_Influencer" in df.columns:
    # Convert column to numeric, forcing errors to NaN for non-numeric values
    df["Daya_Pengaruh_Influencer"] = pd.to_numeric(df["Daya_Pengaruh_Influencer"], errors='coerce')

    # Drop NaN values after conversion (i.e., remove invalid values)
    df_filtered = df.dropna(subset=["Daya_Pengaruh_Influencer"])

    # Count occurrences of each influence level
    influence_counts = df_filtered["Daya_Pengaruh_Influencer"].value_counts().sort_index()

    # Ensure there is valid data before plotting
    if not influence_counts.empty:
        # Plot the strength of influencer impact on followers
        plt.figure(figsize=(10, 6))
        bars = plt.bar(influence_counts.index.astype(str), influence_counts.values, color="darkorange")

        # Add count labels on top of bars
        for bar in bars:
            yval = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)

        # Labels and title
        plt.title("Influence Strength of Influencers on Followers")
        plt.xlabel("Influence Level (1 = Weak, 5 = Strong)")
        plt.ylabel("Number of Followers")
        plt.xticks(rotation=0)
        plt.grid(axis="y", linestyle="--", alpha=0.7)

        # Show plot
        plt.show()
    else:
        print("No valid numerical data found in 'Daya_Pengaruh_Influencer'.")
else:
    print("Column 'Daya_Pengaruh_Influencer' not found in the dataset.")

# Check if the necessary columns exist
if "Kategori_Produk" in df.columns and "Daya_Pengaruh_Influencer" in df.columns:

    # ---- First Visualization: Frequency of Influencers in Each Product Category ----
    category_counts = df["Kategori_Produk"].value_counts()

    plt.figure(figsize=(12, 6))
    bars = plt.bar(category_counts.index, category_counts.values, color="steelblue")

    # Add count labels on top of bars
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)

    plt.title("Frequency of Influencers in Each Product Category")
    plt.xlabel("Product Category")
    plt.ylabel("Number of Influencers")
    plt.xticks(rotation=45, ha="right")
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

# Check if the necessary columns exist
if "Kategori_Produk" in df.columns and "Daya_Pengaruh_Influencer" in df.columns:

    # ---- First Visualization: Frequency of Influencers in Each Product Category ----
    category_counts = df["Kategori_Produk"].value_counts()

    plt.figure(figsize=(12, 6))
    bars = plt.bar(category_counts.index, category_counts.values, color="steelblue")

    # Add count labels on top of bars
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)

    plt.title("Frequency of Influencers in Each Product Category")
    plt.xlabel("Product Category")
    plt.ylabel("Number of Influencers")
    plt.xticks(rotation=45, ha="right")
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

# ---- Second Visualization: Strength of Influencer Impact by Category ----
    # Convert influence column to numeric
    df["Daya_Pengaruh_Influencer"] = pd.to_numeric(df["Daya_Pengaruh_Influencer"], errors='coerce')

    # Group by category and calculate the average influence strength
    category_influence = df.groupby("Kategori_Produk")["Daya_Pengaruh_Influencer"].mean().sort_values(ascending=False)

    plt.figure(figsize=(12, 6))
    bars = plt.bar(category_influence.index, category_influence.values, color="darkorange")

    # Add value labels on top of bars
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom', fontsize=12)

    plt.title("Average Influence Strength of Influencers by Product Category")
    plt.xlabel("Product Category")
    plt.ylabel("Average Influence Strength (1-5 Scale)")
    plt.xticks(rotation=45, ha="right")
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

import pandas as pd
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt

!pip install unidecode
import unidecode

# Initialize lemmatizer and get Indonesian stopwords
lm = WordNetLemmatizer()
stop_words = set(stopwords.words('indonesian'))

# Preprocess the text
final_corpus = []
for i in range(data.shape[0]):
    review = re.sub('[^a-zA-Z]', ' ', data['Opini'][i])
    review = review.lower()
    review = review.split()
    review = [unidecode.unidecode(word) for word in review]
    review = [lm.lemmatize(word) for word in review if not word in stop_words]
    review = ' '.join(review)
    final_corpus.append(review)

# Use CountVectorizer on the preprocessed text
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(final_corpus)

# Calculate word frequencies
sum_words = X.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

# Generate frequency table for top 40 words
df_words_freq = pd.DataFrame(words_freq[:40], columns=['Word', 'Frequency'])

# Convert the frequency list to a dictionary for the word cloud
words_dict = dict(words_freq[:40])

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words_dict)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Initialize lemmatizer and stopwords
lm = WordNetLemmatizer()
stop_words = set(stopwords.words('indonesian'))

# Preprocess the text
final_corpus = []
for review in df["Opini"].dropna():
    review = re.sub('[^a-zA-Z]', ' ', review)  # Remove special characters
    review = review.lower().split()  # Convert to lowercase and split into words
    review = [unidecode.unidecode(word) for word in review]  # Normalize accents
    review = [lm.lemmatize(word) for word in review if word not in stop_words]  # Lemmatize & remove stopwords
    final_corpus.append(' '.join(review))

# Use CountVectorizer on the preprocessed text
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(final_corpus)

# Calculate word frequencies
sum_words = X.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

# Generate frequency table for top words
df_words_freq = pd.DataFrame(words_freq[:40], columns=['Word', 'Frequency'])

print(df_words_freq)

import nltk
import pandas as pd
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt

!pip install unidecode
import unidecode

# Download stopwords (including Indonesian)
nltk.download('stopwords')
nltk.download('punkt')  # This is important for word tokenization
nltk.download('omw-1.4')
nltk.download('wordnet')

# Initialize lemmatizer and get Indonesian stopwords
lm = WordNetLemmatizer()
stop_words = set(stopwords.words('indonesian'))

# Function to preprocess text
def preprocess_text(text):
    review = re.sub('[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters
    review = review.lower().split()  # Convert to lowercase and split into words
    review = [lm.lemmatize(word) for word in review if word not in stop_words]  # Lemmatize & remove stopwords
    return ' '.join(review)


excel_path = "/gab_sc1_dataset3.xlsx"
df = pd.read_excel(excel_path)
data = pd.read_excel(excel_path)

# Check if necessary columns exist
if "Opini" in df.columns and "Label_Sentimen" in df.columns:
    # Preprocess the "Opini" column
    df["cleaned_opini"] = df["Opini"].dropna().apply(preprocess_text)

    # Separate positive and negative sentiment text
    positive_text = " ".join(df[df["Label_Sentimen"] == 1]["cleaned_opini"].dropna())
    negative_text = " ".join(df[df["Label_Sentimen"] == 0]["cleaned_opini"].dropna())

    # Generate Bag of Words for positive and negative sentiments
    vectorizer = CountVectorizer()

    # Positive sentiment analysis
    X_pos = vectorizer.fit_transform([positive_text])
    sum_words_pos = X_pos.sum(axis=0)
    words_freq_pos = [(word, sum_words_pos[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq_pos = sorted(words_freq_pos, key=lambda x: x[1], reverse=True)
    words_dict_pos = dict(words_freq_pos[:40])

    # Negative sentiment analysis
    X_neg = vectorizer.fit_transform([negative_text])
    sum_words_neg = X_neg.sum(axis=0)
    words_freq_neg = [(word, sum_words_neg[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq_neg = sorted(words_freq_neg, key=lambda x: x[1], reverse=True)
    words_dict_neg = dict(words_freq_neg[:40])

    # Generate word clouds
    wordcloud_pos = WordCloud(width=800, height=400, background_color="white", colormap="Greens").generate_from_frequencies(words_dict_pos)
    wordcloud_neg = WordCloud(width=800, height=400, background_color="white", colormap="Reds").generate_from_frequencies(words_dict_neg)

    # Plot the word clouds
    plt.figure(figsize=(12, 6))

    # Positive Sentiment Word Cloud
    plt.subplot(1, 2, 1)
    plt.imshow(wordcloud_pos, interpolation="bilinear")
    plt.axis("off")
    plt.title("Positive Sentiment Word Cloud")

    # Negative Sentiment Word Cloud
    plt.subplot(1, 2, 2)
    plt.imshow(wordcloud_neg, interpolation="bilinear")
    plt.axis("off")
    plt.title("Negative Sentiment Word Cloud")

    # Show the plots
    plt.show()

else:
    print("One or more required columns ('Opini', 'Label_Sentimen') are missing from the dataset.")

from google.colab import drive
drive.mount('/content/drive')

# Manually define a small list of Indonesian stopwords if NLTK stopwords are unavailable
stop_words = {"dan", "di", "yang", "ke", "dari", "untuk", "dengan", "tidak", "ada", "itu", "dalam", "saya"}

# Function to preprocess text
def preprocess_text(text):
    review = re.sub('[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters
    review = review.lower().split()  # Convert to lowercase and split into words
    review = [word for word in review if word not in stop_words]  # Remove stopwords
    return ' '.join(review)


# Check if the "Opini" column exists
if "Opini" in df.columns:
    # Preprocess the "Opini" column
    df["cleaned_opini"] = df["Opini"].dropna().apply(preprocess_text)

    # Initialize TF-IDF Vectorizer
    tfidf_vectorizer = TfidfVectorizer()
    X_tfidf = tfidf_vectorizer.fit_transform(df["cleaned_opini"])  # Fit and transform the text data

    # Convert TF-IDF matrix to DataFrame for better readability
    tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

    # Display the TF-IDF Model as a table
    #print(tfidf_df) # Print the dataframe. You can also use display(tfidf_df) for interactive display in Jupyter Notebooks
    # Instead of using the unknown 'ace_tools', display using pandas:
    display(tfidf_df)  # This will work in Jupyter notebooks


else:
    print("Column 'Opini' not found in the dataset.")

# Count word frequencies in the TF-IDF matrix
word_frequencies = X_tfidf.sum(axis=0).A1

# Create a DataFrame to store words and their frequencies
keywords_df = pd.DataFrame(
    {"Keyword": tfidf_vectorizer.get_feature_names_out(), "Frequency": word_frequencies}
)

# Sort by frequency in descending order
keywords_df = keywords_df.sort_values(by="Frequency", ascending=False)

# Display the top keywords in a table
# Instead of using ace_tools, display the DataFrame using pandas:
# Print the dataframe. You can also use display(keywords_df) for interactive display in Jupyter Notebooks
print(keywords_df)
# OR
display(keywords_df) # This will work in Jupyter notebooks

# Separate positive and negative sentiment text
positive_texts = df[df["Label_Sentimen"] == 1]["cleaned_opini"].dropna()
negative_texts = df[df["Label_Sentimen"] == 0]["cleaned_opini"].dropna()

# Apply TF-IDF transformation separately for positive and negative sentiments
tfidf_vectorizer = TfidfVectorizer()

# TF-IDF for positive sentiment
X_tfidf_pos = tfidf_vectorizer.fit_transform(positive_texts)
word_frequencies_pos = X_tfidf_pos.sum(axis=0).A1
keywords_pos_df = pd.DataFrame(
    {"Keyword": tfidf_vectorizer.get_feature_names_out(), "TF-IDF Score": word_frequencies_pos}
).sort_values(by="TF-IDF Score", ascending=False)

# TF-IDF for negative sentiment
X_tfidf_neg = tfidf_vectorizer.fit_transform(negative_texts)
word_frequencies_neg = X_tfidf_neg.sum(axis=0).A1
keywords_neg_df = pd.DataFrame(
    {"Keyword": tfidf_vectorizer.get_feature_names_out(), "TF-IDF Score": word_frequencies_neg}
).sort_values(by="TF-IDF Score", ascending=False)

# Display the results
# Instead of using ace_tools, use pandas display function
# import ace_tools as tools  # Ensure compatibility with display tools
# tools.display_dataframe_to_user(name="TF-IDF Positive Sentiment", dataframe=keywords_pos_df)
# tools.display_dataframe_to_user(name="TF-IDF Negative Sentiment", dataframe=keywords_neg_df)

# Displaying the dataframes using pandas display function
print("TF-IDF Positive Sentiment")
display(keywords_pos_df)
print("TF-IDF Negative Sentiment")
display(keywords_neg_df)

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# ---- 1. Bar Chart: Top 10 Positive vs. Negative Keywords ----
plt.figure(figsize=(12, 6))

# Select top 10 words for each sentiment
top_positive = keywords_pos_df.head(10)
top_negative = keywords_neg_df.head(10)

# Plot Positive Keywords
plt.barh(top_positive["Keyword"], top_positive["TF-IDF Score"], color="green", label="Positive Sentiment")

# Plot Negative Keywords
plt.barh(top_negative["Keyword"], -top_negative["TF-IDF Score"], color="red", label="Negative Sentiment")

plt.xlabel("TF-IDF Score")
plt.ylabel("Keywords")
plt.title("Comparison of Positive vs Negative Sentiment Keywords")
plt.legend()
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.show()

# ---- 2. Sentiment Distribution by Product Category ----
if "Kategori_Produk" in df.columns:
    sentiment_by_category = df.groupby("Kategori_Produk")["Label_Sentimen"].mean().sort_values()

    plt.figure(figsize=(12, 6))
    sns.barplot(x=sentiment_by_category.index, y=sentiment_by_category.values, palette="coolwarm")
    plt.xticks(rotation=45, ha="right")
    plt.xlabel("Product Category")
    plt.ylabel("Average Sentiment Score (0 = Negative, 1 = Positive)")
    plt.title("Sentiment Distribution by Product Category")
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

# ---- 3. Word Cloud: Positive vs Negative Sentiment ----
plt.figure(figsize=(12, 6))

# Generate Word Clouds
wordcloud_pos = WordCloud(width=400, height=300, background_color="white", colormap="Greens").generate_from_frequencies(dict(top_positive.values))
wordcloud_neg = WordCloud(width=400, height=300, background_color="white", colormap="Reds").generate_from_frequencies(dict(top_negative.values))

# Positive Sentiment Word Cloud
plt.subplot(1, 2, 1)
plt.imshow(wordcloud_pos, interpolation="bilinear")
plt.axis("off")
plt.title("Positive Sentiment Word Cloud")

# Negative Sentiment Word Cloud
plt.subplot(1, 2, 2)
plt.imshow(wordcloud_neg, interpolation="bilinear")
plt.axis("off")
plt.title("Negative Sentiment Word Cloud")

plt.show()

if "Opini" in df.columns:
    # Remove NaN values from "Opini" column
    df_opini = df["Opini"].dropna()

    # Text Preprocessing: Remove non-alphabetic characters and convert to lowercase
    df_opini = df_opini.apply(lambda x: re.sub('[^a-zA-Z]', ' ', x).lower())

    # Initialize CountVectorizer
    vectorizer = CountVectorizer(stop_words='english')  # Removes common stopwords
    X = vectorizer.fit_transform(df_opini)  # Fit and transform the text data

    # Convert to DataFrame for better visualization
    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

    # Display the Bag of Words Model as a structured table
    tools.display_dataframe_to_user(name="Bag of Words Model", dataframe=bow_df)

else:
    print("Column 'Opini' not found in the dataset.")
